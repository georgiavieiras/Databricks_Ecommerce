# ğŸ… Projeto Databricks - Arquitetura Medallion E-commerce

## ğŸ“‹ VisÃ£o Geral do Projeto

Este projeto foi desenvolvido para ensinar os conceitos da **Arquitetura Medallion** no Databricks, aplicada a um case real de e-commerce. VocÃª atuarÃ¡ como **Engenheiro de Dados** e deverÃ¡ construir um pipeline de dados completo, desde a ingestÃ£o bruta atÃ© a entrega de mÃ©tricas de negÃ³cio.

### ğŸ¯ Objetivos de Aprendizagem

- Implementar a arquitetura medallion (Bronze â†’ Silver â†’ Gold)
- Aplicar conceitos de qualidade de dados
- Criar transformaÃ§Ãµes incrementais e otimizadas
- Gerar mÃ©tricas de negÃ³cio prontas para consumo
- Utilizar PySpark e SQL no Databricks

---

## ğŸ—ï¸ Arquitetura Medallion

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BRONZE    â”‚  â†’   â”‚   SILVER    â”‚  â†’   â”‚    GOLD     â”‚
â”‚ (Raw Data)  â”‚      â”‚  (Refined)  â”‚      â”‚ (Business)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                     â”‚                     â”‚
     â”‚                     â”‚                     â”‚
  IngestÃ£o          Limpeza/PadronizaÃ§Ã£o    AgregaÃ§Ãµes
  ValidaÃ§Ã£o         Joins/Enrichment         KPIs/MÃ©tricas
  HistÃ³rico         DeduplicaÃ§Ã£o             Dashboards
```

### Camadas

| Camada | Objetivo | TransformaÃ§Ãµes |
|--------|----------|----------------|
| **Bronze** | Dados brutos (raw) | MÃ­nima transformaÃ§Ã£o, validaÃ§Ã£o bÃ¡sica |
| **Silver** | Dados limpos e confiÃ¡veis | Limpeza, padronizaÃ§Ã£o, joins, deduplicaÃ§Ã£o |
| **Gold** | MÃ©tricas de negÃ³cio | AgregaÃ§Ãµes, KPIs, modelos dimensionais |

---

## ğŸ¥‰ BRONZE LAYER - IngestÃ£o e ValidaÃ§Ã£o

### Contexto
VocÃª recebeu dados brutos em formato CSV/JSON de um sistema legado de e-commerce. Sua missÃ£o Ã© ingerir esses dados no Data Lake mantendo a rastreabilidade e aplicando validaÃ§Ãµes bÃ¡sicas.

### Tabelas de Origem
- `categorias` - Categorias de produtos
- `produtos` - CatÃ¡logo de produtos
- `clientes` - Base de clientes
- `pedidos` - HistÃ³rico de pedidos
- `itens_pedido` - Itens dos pedidos
- `pagamentos` - TransaÃ§Ãµes de pagamento
- `avaliacoes` - AvaliaÃ§Ãµes de produtos

---

### ğŸ“ Desafios Bronze

<details>
<summary><strong>Desafio B1 - IngestÃ£o com Auditoria</strong></summary>

**Objetivo:** Criar uma funÃ§Ã£o de ingestÃ£o genÃ©rica que adicione metadados de auditoria

**Requisitos:**
- Ler arquivos CSV da camada landing
- Adicionar colunas de auditoria:
  - `data_ingestao` (timestamp da ingestÃ£o)
  - `arquivo_origem` (nome do arquivo fonte)
- Salvar no formato Delta na camada bronze

**Tabelas:** Todas (categorias, produtos, clientes, pedidos, itens_pedido, pagamentos, avaliacoes)

</details>

<details>

---

## ğŸ¥ˆ SILVER LAYER - Limpeza e PadronizaÃ§Ã£o

### Contexto
Agora que os dados brutos estÃ£o no bronze, vocÃª precisa criar tabelas confiÃ¡veis e prontas para anÃ¡lise. Essa camada serÃ¡ consumida por cientistas de dados e analistas.

---

### ğŸ“ Desafios Silver

<details>
<summary><strong>Desafio S1 - Limpeza e PadronizaÃ§Ã£o</strong></summary>

**Objetivo:** Limpar e padronizar os dados bronze

**TransformaÃ§Ãµes por tabela:**

**Produtos:**
- Remover espaÃ§os em branco extras em `nome` e `marca`
- Padronizar `marca` para Title Case (Dell, Samsung, Apple)
- Converter `preco` para decimal(10,2)
- Adicionar coluna `faixa_preco`: 'EconÃ´mico' (<100), 'MÃ©dio' (100-500), 'Premium' (>500)
- Filtrar apenas produtos ativos e vÃ¡lidos

**Clientes:**
- Padronizar `nome` para Title Case
- Converter `email` para lowercase
- Formatar `telefone` no padrÃ£o (XX) XXXXX-XXXX
- Criar coluna `regiao` baseada no estado:
  - Sudeste: SP, RJ, MG, ES
  - Sul: PR, SC, RS
  - Nordeste: BA, PE, CE, RN, PB, AL, SE, MA, PI
  - Norte: AM, PA, AC, RO, RR, AP, TO
  - Centro-Oeste: GO, MT, MS, DF

**Pedidos:**
- Calcular `valor_liquido = valor_total - desconto`
- Criar coluna `trimestre` e `ano` a partir de `data_pedido`
- Classificar `ticket`: 'Baixo' (<200), 'MÃ©dio' (200-1000), 'Alto' (>1000)

</details>

<details>
<summary><strong>Desafio S2 - Enriquecimento com Joins</strong></summary>

**Objetivo:** Criar visÃµes enriquecidas combinando tabelas

**Criar as seguintes tabelas silver:**

**`silver_pedidos_completos`:**
- Juntar: pedidos + clientes + itens_pedido + produtos + categorias + pagamentos
- Incluir:
  - Dados do cliente (nome, cidade, estado, regiao)
  - Dados do produto (nome, categoria, marca)
  - Status do pagamento
  - Quantidade de itens no pedido
  - MÃ©todo de pagamento

**`silver_produtos_enriquecidos`:**
- Juntar: produtos + categorias + avaliacoes
- Calcular:
  - MÃ©dia de avaliaÃ§Ãµes por produto
  - Quantidade total de avaliaÃ§Ãµes
  - Categoria do produto

**`silver_clientes_consolidados`:**
- Juntar: clientes + pedidos + avaliacoes
- Calcular:
  - Total gasto por cliente (lifetime value)
  - Quantidade de pedidos
  - Ticket mÃ©dio
  - Data do primeiro pedido
  - Data do Ãºltimo pedido
  - Produtos avaliados

</details>

---

## ğŸ¥‡ GOLD LAYER - MÃ©tricas de NegÃ³cio

### Contexto
A camada Gold Ã© consumida diretamente por ferramentas de BI (Power BI, Tableau) e pela diretoria. As tabelas devem estar otimizadas, agregadas e responder perguntas de negÃ³cio especÃ­ficas.

---

### ğŸ“ Desafios Gold

<details>
<summary><strong>Desafio G1 - KPIs de Vendas</strong></summary>

**Objetivo:** Criar tabela `gold_kpis_vendas` com visÃ£o diÃ¡ria/mensal/anual

**MÃ©tricas:**
- Receita total (bruta e lÃ­quida)
- Ticket mÃ©dio
- Quantidade de pedidos
- Quantidade de itens vendidos
- Desconto total concedido
- Frete total
- Taxa de conversÃ£o (pedidos entregues / total)
- AOV (Average Order Value)
- Receita por regiÃ£o

**Granularidades:**
- DiÃ¡ria (`gold_kpis_vendas_diario`)
- Mensal (`gold_kpis_vendas_mensal`)
- Anual (`gold_kpis_vendas_anual`)

**Dica:** Use window functions para calcular variaÃ§Ã£o % vs perÃ­odo anterior

</details>

<details>
<summary><strong>Desafio G2 - AnÃ¡lise de Produtos</strong></summary>

**Objetivo:** Criar `gold_produtos_performance`

**MÃ©tricas por produto:**
- Quantidade vendida
- Receita gerada
- Ticket mÃ©dio
- FrequÃªncia de compra
- AvaliaÃ§Ã£o mÃ©dia
- Taxa de avaliaÃ§Ã£o (avaliaÃ§Ãµes / vendas)
- Estoque atual vs mÃ©dia de vendas (dias de estoque)
- Ranking de vendas (geral e por categoria)
- % de participaÃ§Ã£o na receita
- Produtos mais vendidos em combo

**SegmentaÃ§Ãµes:**
- Por categoria
- Por faixa de preÃ§o
- Por marca

</details>

<details>
<summary><strong>Desafio G4 - AnÃ¡lise de Coorte</strong></summary>

**Objetivo:** Criar `gold_cohort_analysis` analisando retenÃ§Ã£o de clientes

**DefiniÃ§Ã£o:**
- Coorte = mÃªs da primeira compra
- AnÃ¡lise de retenÃ§Ã£o mensal

**MÃ©tricas:**
- Taxa de retenÃ§Ã£o mÃªs a mÃªs
- Receita por coorte ao longo do tempo
- Quantidade de clientes ativos por coorte
- LTV (Lifetime Value) por coorte
- Churn rate por coorte

**Formato da tabela:**
```
cohort_mes | mes_0 | mes_1 | mes_2 | mes_3 | ...
2024-01    | 100%  | 45%   | 32%   | 28%   | ...
2024-02    | 100%  | 50%   | 35%   | ...   | ...
```

</details>

<details>
<summary><strong>Desafio G5 - Market Basket Analysis</strong></summary>

**Objetivo:** Identificar produtos frequentemente comprados juntos

**Criar `gold_product_affinity`:**
- Pares de produtos comprados juntos
- Support: % de pedidos que contÃ©m o par
- Confidence: P(produto_B | produto_A)
- Lift: Confidence / P(produto_B)

**MÃ©tricas:**
- Top 20 combinaÃ§Ãµes mais frequentes
- SugestÃµes de cross-sell por produto
- Produtos que raramente sÃ£o comprados sozinhos

**Entrega:**
- Tabela com pares de produtos e mÃ©tricas
- Filtrar apenas pares com support > 1% e lift > 1

</details>

<details>
<summary><strong>Desafio G6 - AnÃ¡lise de Pagamentos</strong></summary>

**Objetivo:** Criar `gold_pagamentos_analysis`

**MÃ©tricas:**
- Taxa de aprovaÃ§Ã£o por mÃ©todo de pagamento
- Ticket mÃ©dio por mÃ©todo
- Tempo mÃ©dio de aprovaÃ§Ã£o
- Taxa de fraude (pagamentos recusados)
- PreferÃªncia de pagamento por regiÃ£o
- EvoluÃ§Ã£o de mÃ©todos ao longo do tempo

**AnÃ¡lises:**
- MÃ©todo mais usado por faixa de valor
- CorrelaÃ§Ã£o entre mÃ©todo e taxa de cancelamento
- Sazonalidade nos mÃ©todos de pagamento

</details>

<details>
<summary><strong>Desafio G7 - AnÃ¡lise de Churn</strong></summary>

**Objetivo:** Criar `gold_churn_prediction`

**Definir churn:**
- Cliente sem compras nos Ãºltimos 90 dias (considerando frequÃªncia histÃ³rica)

**Calcular features:**
- Dias desde Ãºltima compra
- MÃ©dia de dias entre compras
- TendÃªncia de compra (crescente/decrescente)
- Valor da Ãºltima compra vs mÃ©dia
- Quantidade de avaliaÃ§Ãµes deixadas
- Taxa de cancelamento histÃ³rica

**Entrega:**
- Tabela com cliente_id e flag `em_risco_churn`
- Score de propensÃ£o ao churn (0-100)
- Segmento de risco (baixo, mÃ©dio, alto)

</details>

<details>
<summary><strong>Desafio G8 - Dashboard Executivo</strong></summary>

**Objetivo:** Criar `gold_executive_dashboard` - Snapshot diÃ¡rio

**MÃ©tricas em tempo real:**
- Receita do dia/mÃªs/ano
- Pedidos do dia/mÃªs/ano
- Ticket mÃ©dio
- Top 5 produtos do mÃªs
- Top 5 categorias do mÃªs
- Top 5 clientes do mÃªs
- Novos clientes do mÃªs
- Taxa de conversÃ£o
- NPS (Net Promoter Score) baseado em avaliaÃ§Ãµes
- Comparativo com mesmo perÃ­odo do ano anterior

**Formato:**
- Uma linha por dia
- Otimizada para visualizaÃ§Ã£o em dashboard

</details>

---

## ğŸ¯ Desafio Final Integrado

<details>
<summary><strong>ğŸ† Boss Final - Pipeline Completo End-to-End</strong></summary>

### Contexto
VocÃª foi promovido a **Lead Data Engineer** e precisa entregar um pipeline completo de dados para o CEO do e-commerce. Ele quer respostas para as seguintes perguntas estratÃ©gicas:

### Perguntas de NegÃ³cio

1. **Qual categoria de produto tem maior margem e deve receber investimento em marketing?**
   - Considere: receita, ticket mÃ©dio, avaliaÃ§Ãµes, frequÃªncia de recompra

2. **Quais clientes devemos focar em retenÃ§Ã£o urgente?**
   - Identifique clientes de alto valor em risco de churn
   - Calcule o impacto financeiro da perda desses clientes

3. **Existe oportunidade de cross-sell e upsell?**
   - Identifique padrÃµes de compra
   - Sugira produtos complementares
   - Calcule potencial de receita adicional

4. **Nossa operaÃ§Ã£o de frete estÃ¡ otimizada?**
   - Analise correlaÃ§Ã£o entre frete e cancelamento
   - Identifique regiÃµes com maior custo de frete
   - Sugira otimizaÃ§Ãµes

5. **Qual o perfil do nosso cliente ideal (ICP)?**
   - Defina baseado em: regiÃ£o, ticket, frequÃªncia, produtos comprados
   - Compare com clientes de baixo valor

6. **Como estÃ¡ nossa saÃºde financeira mÃªs a mÃªs?**
   - TendÃªncias de crescimento
   - Sazonalidade
   - PrevisÃ£o para prÃ³ximos 3 meses (usar mÃ©dia mÃ³vel)

### Requisitos TÃ©cnicos

**Pipeline deve incluir:**
- âœ… IngestÃ£o incremental (processar apenas dados novos)
- âœ… ValidaÃ§Ã£o de qualidade em cada camada
- âœ… Logging e monitoramento
- âœ… OtimizaÃ§Ã£o de performance (particionamento, Z-order, vacuum)
- âœ… Testes automatizados (Great Expectations ou similar)
- âœ… DocumentaÃ§Ã£o das tabelas (data catalog)
- âœ… OrquestraÃ§Ã£o (Databricks Workflows ou Airflow)

**Entrega:**
1. Notebooks organizados por camada (bronze/silver/gold)
2. Tabelas Gold respondendo cada pergunta de negÃ³cio
3. README.md explicando arquitetura e como executar
4. Dashboard visual (Databricks SQL ou Power BI)
5. ApresentaÃ§Ã£o executiva (10 slides) com insights

### CritÃ©rios de AvaliaÃ§Ã£o

| CritÃ©rio | Peso |
|----------|------|
| Qualidade do cÃ³digo (PEP8, docstrings) | 15% |
| Modelagem de dados (normalizaÃ§Ã£o, performance) | 20% |
| Qualidade e validaÃ§Ã£o dos dados | 20% |
| Insights de negÃ³cio gerados | 25% |
| DocumentaÃ§Ã£o e organizaÃ§Ã£o | 10% |
| OtimizaÃ§Ã£o e performance | 10% |

</details>

---

## ğŸ“š Recursos Adicionais

### Conceitos Importantes

**Delta Lake:**
- Time Travel
- ACID Transactions
- Schema Evolution
- MERGE operations

**OtimizaÃ§Ã£o:**
- Z-Order
- Particionamento
- Vacuum
- Optimize

**Qualidade de Dados:**
- Great Expectations
- Data validation rules
- Anomaly detection

**OrquestraÃ§Ã£o:**
- Databricks Workflows
- Apache Airflow
- Task dependencies

### Boas PrÃ¡ticas

1. **Nomenclatura:**
   - `bronze_<nome_tabela>` para dados brutos
   - `silver_<nome_tabela>` para dados limpos
   - `gold_<metrica>_<granularidade>` para agregaÃ§Ãµes

2. **Particionamento:**
   - Bronze: `data_ingestao`
   - Silver: `ano` e `mes`
   - Gold: `data_referencia`

3. **Incrementalidade:**
   - Use watermarks para processar apenas novos dados
   - Mantenha controle de Ãºltima execuÃ§Ã£o

4. **Performance:**
   - Cache tabelas pequenas e frequentemente usadas
   - Use broadcast joins quando apropriado
   - Particione antes de joins grandes

---

## ğŸš€ Como ComeÃ§ar

1. **Setup do Ambiente:**
   - Criar workspace Databricks (Community Edition ou trial)
   - Configurar cluster
   - Upload dos dados CSV

2. **Estrutura de Pastas:**
```
/dbfs/mnt/datalake/
â”œâ”€â”€ landing/          # Dados brutos (CSV/JSON)
â”œâ”€â”€ bronze/          # Delta tables - raw
â”œâ”€â”€ silver/          # Delta tables - refined
â””â”€â”€ gold/            # Delta tables - business
```

3. **Ordem de ExecuÃ§Ã£o:**
   - Comece pela camada Bronze (ingestÃ£o)
   - Avance para Silver (limpeza)
   - Finalize com Gold (mÃ©tricas)
   - Desenvolva o pipeline incremental

4. **IteraÃ§Ã£o:**
   - FaÃ§a um desafio por vez
   - Valide os resultados
   - Otimize conforme necessÃ¡rio

---

## âœ… Checklist de ConclusÃ£o

### Bronze
- [ ] IngestÃ£o com auditoria implementada
- [ ] ValidaÃ§Ãµes de integridade funcionando
- [ ] Duplicatas identificadas
- [ ] Dashboard de qualidade criado

### Silver
- [ ] Dados limpos e padronizados
- [ ] Joins e enriquecimentos realizados
- [ ] SCD Tipo 2 implementado
- [ ] Anomalias detectadas
- [ ] DeduplicaÃ§Ã£o aplicada

### Gold
- [ ] KPIs de vendas calculados
- [ ] Performance de produtos analisada
- [ ] AnÃ¡lise RFM concluÃ­da
- [ ] AnÃ¡lise de coorte implementada
- [ ] Market basket analysis feita
- [ ] AnÃ¡lise de pagamentos completa
- [ ] Modelo de churn criado
- [ ] Dashboard executivo funcionando

### Desafio Final
- [ ] Pipeline end-to-end implementado
- [ ] Todas as perguntas de negÃ³cio respondidas
- [ ] DocumentaÃ§Ã£o completa
- [ ] Dashboard visual criado
- [ ] ApresentaÃ§Ã£o preparada

---

## ğŸ“ PrÃ³ximos Passos

ApÃ³s concluir este projeto, vocÃª estarÃ¡ preparado para:

1. **Trabalhar com Data Lakes modernos**
2. **Implementar arquiteturas de dados escalÃ¡veis**
3. **Aplicar engenharia de dados em casos reais**
4. **Gerar valor atravÃ©s de dados**
5. **AvanÃ§ar para Machine Learning e IA**

### SugestÃµes de EvoluÃ§Ã£o

- Implementar streaming com Structured Streaming
- Adicionar camada de feature store para ML
- Criar pipelines de CI/CD para cÃ³digo
- Implementar data quality com Great Expectations
- Orquestrar com Airflow ou Databricks Workflows
- Adicionar governanÃ§a com Unity Catalog

---

**Boa sorte e bons estudos! ğŸš€**

*Lembre-se: dados de qualidade sÃ£o a base de qualquer decisÃ£o inteligente.*
